{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "df = pd.read_csv('headlines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------***-----------------------\n",
      "   publish_date                                      headline_text\n",
      "0      20170721  algorithms can make decisions on behalf of fed...\n",
      "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2      20170721                           a rural mural in thallan\n",
      "3      20170721  australia church risks becoming haven for abusers\n",
      "4      20170721  australian company usgfx embroiled in shanghai...\n",
      "-----------------------***-----------------------\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170721</td>\n",
       "      <td>algorithms can make decisions on behalf of fed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170721</td>\n",
       "      <td>andrew forrests fmg to appeal pilbara native t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170721</td>\n",
       "      <td>a rural mural in thallan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australia church risks becoming haven for abusers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170721</td>\n",
       "      <td>australian company usgfx embroiled in shanghai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20170721  algorithms can make decisions on behalf of fed...\n",
       "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
       "2      20170721                           a rural mural in thallan\n",
       "3      20170721  australia church risks becoming haven for abusers\n",
       "4      20170721  australian company usgfx embroiled in shanghai..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Have a look at the data\n",
    "\n",
    "print(\"-----------------------***-----------------------\")\n",
    "print(df.head(5))\n",
    "print(\"-----------------------***-----------------------\")\n",
    "print()\n",
    "df.info()\n",
    "print()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [algorithm, make, decis, behalf, feder, minist]\n",
       "1       [andrew, forrest, fmg, appeal, pilbara, nativ,...\n",
       "2                                 [rural, mural, thallan]\n",
       "3                  [australia, church, risk, becom, abus]\n",
       "4       [australian, compani, usgfx, embroil, shanghai...\n",
       "                              ...                        \n",
       "1994    [constitut, avenu, win, top, prize, act, archi...\n",
       "1995                         [dark, mofo, number, crunch]\n",
       "1996    [david, petraeu, say, australia, must, firm, s...\n",
       "1997    [driverless, car, australia, face, challeng, r...\n",
       "1998               [drug, compani, criticis, price, hike]\n",
       "Name: stemmed, Length: 1999, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Perform preprocessing\n",
    "# import needed modules\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "# df_original = df # no use pointer used as objects\n",
    "df['tokens'] = df.apply(lambda row: nltk.word_tokenize(row['headline_text']),axis=1)\n",
    "# df_tokenised = df['tokens'] # no use pointer used as objects\n",
    "# df_tokenised = df # no use pointer used as objects\n",
    "\n",
    "# Remove punctuation\n",
    "df['alpha'] = df['tokens'].apply(lambda x: [item for item in x if item.isalpha()])\n",
    "# df_alpha = df # no use pointer used as objects\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "df['stop'] = df['alpha'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "#df # \"on removed\"\n",
    "\n",
    "# Stem\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['stop'].apply(lambda x: [stemmer.stem(item) for item in x])\n",
    "df['stemmed']\n",
    "#df['stop']\n",
    "#df['alpha']\n",
    "#df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 4165)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the BOW of the preprocessed data\n",
    "\n",
    "from sklearn .feature_extraction.text import CountVectorizer # counts up all the tokens inside the text\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=False,analyzer = lambda x: x)\n",
    "BOW = vectorizer.fit_transform(df['stemmed']).toarray()\n",
    "BOW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aardman</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abel</th>\n",
       "      <th>abil</th>\n",
       "      <th>ablett</th>\n",
       "      <th>aborigin</th>\n",
       "      <th>...</th>\n",
       "      <th>youtub</th>\n",
       "      <th>zambian</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zedd</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zion</th>\n",
       "      <th>zombi</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonta</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999 rows × 4165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aardman  aaron  ab  aback  abbott  abc  abel  abil  ablett  aborigin  \\\n",
       "0           0      0   0      0       0    0     0     0       0         0   \n",
       "1           0      0   0      0       0    0     0     0       0         0   \n",
       "2           0      0   0      0       0    0     0     0       0         0   \n",
       "3           0      0   0      0       0    0     0     0       0         0   \n",
       "4           0      0   0      0       0    0     0     0       0         0   \n",
       "...       ...    ...  ..    ...     ...  ...   ...   ...     ...       ...   \n",
       "1994        0      0   0      0       0    0     0     0       0         0   \n",
       "1995        0      0   0      0       0    0     0     0       0         0   \n",
       "1996        0      0   0      0       0    0     0     0       0         0   \n",
       "1997        0      0   0      0       0    0     0     0       0         0   \n",
       "1998        0      0   0      0       0    0     0     0       0         0   \n",
       "\n",
       "      ...  youtub  zambian  zealand  zedd  zinc  zion  zombi  zone  zonta  zoo  \n",
       "0     ...       0        0        0     0     0     0      0     0      0    0  \n",
       "1     ...       0        0        0     0     0     0      0     0      0    0  \n",
       "2     ...       0        0        0     0     0     0      0     0      0    0  \n",
       "3     ...       0        0        0     0     0     0      0     0      0    0  \n",
       "4     ...       0        0        0     0     0     0      0     0      0    0  \n",
       "...   ...     ...      ...      ...   ...   ...   ...    ...   ...    ...  ...  \n",
       "1994  ...       0        0        0     0     0     0      0     0      0    0  \n",
       "1995  ...       0        0        0     0     0     0      0     0      0    0  \n",
       "1996  ...       0        0        0     0     0     0      0     0      0    0  \n",
       "1997  ...       0        0        0     0     0     0      0     0      0    0  \n",
       "1998  ...       0        0        0     0     0     0      0     0      0    0  \n",
       "\n",
       "[1999 rows x 4165 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08333333, 0.09090909, 0.1       , 0.11111111,\n",
       "       0.125     , 0.14285714, 0.16666667, 0.18181818, 0.2       ,\n",
       "       0.22222222, 0.25      , 0.28571429, 0.33333333, 0.4       ,\n",
       "       0.5       , 1.        ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the TF using the BOW\n",
    "TF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names())\n",
    "TF = TF.divide(TF.sum(axis=1),axis=0) # axis=0, perform calculations over the columns\n",
    "#TF\n",
    "np.unique(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.28291422, 3.36629583, 3.44151925, 3.53995932, 3.57505064,\n",
       "       3.70858204, 3.79373984, 3.83920222, 3.91152288, 3.96281617,\n",
       "       4.04505427, 4.10389477, 4.13466643, 4.16641513, 4.19920495,\n",
       "       4.2331065 , 4.26819782, 4.30456547, 4.3423058 , 4.38152651,\n",
       "       4.4223485 , 4.46490812, 4.50935988, 4.5558799 , 4.60467006,\n",
       "       4.65596336, 4.71003058, 4.76718899, 4.82781361, 4.89235213,\n",
       "       4.961345  , 5.03545298, 5.11549568, 5.20250706, 5.29781724,\n",
       "       5.40317776, 5.52096079, 5.65449219, 5.80864287, 5.99096442,\n",
       "       6.21410797, 6.50179005, 6.90725515, 7.60040233])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the IDF\n",
    "\n",
    "IDF = pd.DataFrame(data=BOW, columns=vectorizer.get_feature_names())\n",
    "IDF[IDF>1] = 1\n",
    "IDF = np.log(len(IDF)/IDF.sum(axis=0))\n",
    "\n",
    "np.unique(IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the TF-IDF\n",
    "\n",
    "#TF\n",
    "#IDF\n",
    "\n",
    "tfidf = TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words: gcfc    0.633367\n",
      "geel    0.633367\n",
      "gw      0.633367\n",
      "haw     0.633367\n",
      "melb    0.633367\n",
      "coll    0.633367\n",
      "adel    0.633367\n",
      "syd     0.633367\n",
      "nmfc    0.633367\n",
      "cold    0.690456\n",
      "dtype: float64\n",
      "highest words: peacemak     7.600402\n",
      "pump         6.907255\n",
      "date         3.800201\n",
      "puffbal      3.800201\n",
      "superannu    3.800201\n",
      "mongolian    3.800201\n",
      "loophol      3.800201\n",
      "rig          3.800201\n",
      "aquapon      3.800201\n",
      "mous         3.800201\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "\n",
    "print(\"lowest words:\", tfidf.max(axis=0).sort_values()[:10])\n",
    "print(\"highest words:\", tfidf.max(axis=0).sort_values(ascending=False)[:10]) # Values are the same as the tutorial instructors but not as the initial values shown in the original version of this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words: coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "highest words: peacemak     1.000000\n",
      "pump         1.000000\n",
      "mongolian    0.831769\n",
      "financ       0.803629\n",
      "employ       0.795060\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "travel       0.788050\n",
      "rig          0.786813\n",
      "mosul        0.779137\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the TF-IDF using scikit learn\n",
    "# Import the module\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the TF-IDF vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False,analyzer=lambda x:x)\n",
    "\n",
    "# Compute the TF-IDF\n",
    "\n",
    "tf_idf = vectorizer.fit_transform(df['stemmed']).toarray()\n",
    "tf_idf = pd.DataFrame(data=tf_idf, columns=vectorizer.get_feature_names()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest words: coll     0.305258\n",
      "gw       0.305258\n",
      "nmfc     0.305258\n",
      "adel     0.305258\n",
      "melb     0.305258\n",
      "syd      0.305258\n",
      "haw      0.305258\n",
      "geel     0.305258\n",
      "gcfc     0.305258\n",
      "fabio    0.322574\n",
      "dtype: float64\n",
      "highest words: peacemak     1.000000\n",
      "pump         1.000000\n",
      "mongolian    0.831769\n",
      "financ       0.803629\n",
      "employ       0.795060\n",
      "aquapon      0.794899\n",
      "date         0.794899\n",
      "travel       0.788050\n",
      "rig          0.786813\n",
      "mosul        0.779137\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 10 words with the highest and lowest TF-IDF on average\n",
    "\n",
    "print(\"lowest words:\", tf_idf.max(axis=0).sort_values()[:10])\n",
    "print(\"highest words:\", tf_idf.max(axis=0).sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Cell values are normalised between 0 and 1 (like in porobility mathematics, where the scale is 0 to 1)\n",
    "0 - no correlation\n",
    "1 - exact correlation or ratio\n",
    "\n",
    "All the words represented as vectors will reatain their ratios with repect to the sentences. However the normalised TF-IDF calculatons will assist in determining the relative distribution of words accross sentences with respect to other words, this is ideal for plotting and comparing the prevelnce of words overall.\n",
    "\n",
    "However the first calculation usinf \"tfidf = TF * IDF\", in the first case is better in obtaining the number of representation of words in accending or decencding oreder is the analyst is interested in the highest or lowest prevelance of words for the number of sentances containing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
