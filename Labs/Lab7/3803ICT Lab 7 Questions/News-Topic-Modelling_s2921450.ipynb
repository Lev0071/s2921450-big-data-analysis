{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling for News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1495020689067-958852a7765e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Roman Kraft](https://unsplash.com/photos/_Zua2hyvTBk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about modelling the main topics of a database of News headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in the file `random_headlines.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120305</td>\n",
       "      <td>ute driver hurt in intersection crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20081128</td>\n",
       "      <td>6yo dies in cycling accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090325</td>\n",
       "      <td>bumper olive harvest expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20100201</td>\n",
       "      <td>replica replaces northernmost sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080225</td>\n",
       "      <td>woods targets perfect season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                          headline_text\n",
       "0      20120305  ute driver hurt in intersection crash\n",
       "1      20081128           6yo dies in cycling accident\n",
       "2      20090325          bumper olive harvest expected\n",
       "3      20100201     replica replaces northernmost sign\n",
       "4      20080225           woods targets perfect season"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: load the dataset\n",
    "df = pd.read_csv(\"random_headlines.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is always a good idea to perform some EDA (exploratory data analytics) on a dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   20000 non-null  int64 \n",
      " 1   headline_text  20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform a short EDA\n",
    "df.info()\n",
    "# both columns contain non-null values\n",
    "# second column is object (or string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform all the needed preprocessing on those headlines: case lowering, tokenization, punctuation removal, stopwords removal, stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ute, driver, hurt, intersect, crash]\n",
       "1                       [die, cycl, accid]\n",
       "2          [bumper, oliv, harvest, expect]\n",
       "3    [replica, replac, northernmost, sign]\n",
       "4          [wood, target, perfect, season]\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Preprocess the input data\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenise\n",
    "df['tokens'] = df[\"headline_text\"].apply(lambda saf: nltk.word_tokenize(saf))\n",
    "\n",
    "# punctuation\n",
    "df['alphaNumeric'] = df['tokens'].apply(lambda saf: [wrd for wrd in saf if wrd.isalpha()])\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# remove stopwords\n",
    "stop_word = nltk.corpus.stopwords.words(\"english\")\n",
    "df['stop'] = df['alphaNumeric'].apply(lambda row: [ word for word in row if word not in stop_word ])\n",
    "\n",
    "# df.head()\n",
    "# # in removed\n",
    "\n",
    "# stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "df['stemmed'] = df['stop'].apply(lambda row: [stemmer.stem(word) for word in row])\n",
    "\n",
    "df['stemmed'].head()\n",
    "# accident -> accid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Gensim to compute a BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the BOW using Gensim\n",
    "\n",
    "# class gensim.corpora.dictionary.Dictionary(Documents=None,prune_at=2000000)\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(df['stemmed'])\n",
    "corpus = [dictionary.doc2bow(line) for line in df['stemmed']]\n",
    "print(np.shape(corpus))\n",
    "print(corpus[0:2])\n",
    "# dictionary.doc2bow([word_1,word_2,...,word_n]) # pass each array line by line\n",
    "# we have 20,00 documents inside the corpus\n",
    "    # for each document there is this (x,y) BOW representation\n",
    "    # x - id of the word (essentially a dictionary)\n",
    "    # y - count of that word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the TF-IDF using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute TF-IDF\n",
    "\n",
    "#from gensim.models import TfidfModel\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tf_idf = tfidf_model[corpus]\n",
    "print(np.shape(tf_idf))\n",
    "# returns an article of 20,000 articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally compute the **LSA** (also called LSI) using Gensim, for a given number of Topics that you choose yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute LSA\n",
    "\n",
    "# Google -- Latent Semantic Analysis site:radimrehurek.com\n",
    "\n",
    "# Try to calculate some of the most common topic that appear indie the articles\n",
    "# here we care about (and obtain) the top 4 topics\n",
    "\n",
    "# Latent Symantic Indexing\n",
    "\n",
    "#------\n",
    "#Eg\n",
    "# run distributed LSA on nine documents\n",
    "# lsi = models.LsiModel(corpus, id2word=id2word, num_topics=200, chunksize=1, distributed=True)\n",
    "#------\n",
    "\n",
    "# from gensim.models import LsiModel\n",
    "# lsi = LsiModel(corpus=corpus, num_topics=4,id2word=dictionary)\n",
    "\n",
    "from gensim import corpora, models\n",
    "#lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=4, chunksize=1, distributed=True)\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the topic, show the most significant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '-0.752*\"polic\" + -0.404*\"man\" + -0.208*\"charg\"'),\n",
       " (1, '0.670*\"man\" + -0.574*\"polic\" + 0.328*\"charg\"'),\n",
       " (2, '0.654*\"new\" + 0.296*\"plan\" + -0.242*\"man\"'),\n",
       " (3, '0.703*\"new\" + -0.346*\"say\" + -0.334*\"plan\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Print the 3 or 4 most significant words of each topic\n",
    "\n",
    "lsi.print_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about those results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the two dictionary entries 0 and 1 that seem to have the context of crimes news reports as these two documents conatins term associated with criminal incidents.\n",
    "\n",
    "Documents 2 and 3 of the corpus contain terminilogies associated with politics\n",
    "\n",
    "At this stage of my learning I am assuming \"politics\" and \"crime\" may possibly be classes in the bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use LDA instead of LSA using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute LDA\n",
    "\n",
    "# Google -- Latent dirichlet Allocaton site:radimrehurek.com\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "lda = LdaModel(corpus=corpus, num_topics=4, id2word=dictionary,random_state=0,chunksize=512,passes=5)\n",
    "# random_state - there is a degrre of randomness in the function, this parameter ensures it is always the same\n",
    "# chunksize - Number of documents to be used in each training chunk. Similiar to the *0% & 20% we specified in previous labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.016*\"report\" + 0.009*\"back\" + 0.009*\"may\"'),\n",
       " (1, '0.012*\"mine\" + 0.011*\"polic\" + 0.009*\"elect\"'),\n",
       " (2, '0.013*\"question\" + 0.010*\"council\" + 0.010*\"fund\"'),\n",
       " (3, '0.012*\"sydney\" + 0.012*\"charg\" + 0.011*\"australian\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: print the most frequent words of each topic\n",
    "lda.print_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how does it work with LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way the two models LSI and LSA are fundamentally different, but shres similarities.\n",
    "LSI identifies a set of concepts from a corpus. On the Other hand LDA considers context words and topics.\n",
    "\n",
    "Both methods use bag of words, however LSA focuses on reducing matrix dimension while LDA solves topic modelling problems.\n",
    "Hence why the output of LDA here seems more meaningful to the user as it has more contrast and is less abstract compared to the results of LSI analysis.\n",
    "\n",
    "\n",
    "LSI\n",
    "----\n",
    "\n",
    "\n",
    "Find common themes in documents \n",
    "Map\n",
    " document -> concept\n",
    " term -> concept\n",
    " concept -- set of terms with weights eg. \"data\"(0.8)\n",
    " \n",
    "like an automatically constructed thesaurus (wrt to words mapping to concepts)\n",
    "\n",
    "Discussion\n",
    " to retrieve concepts from documents\n",
    " to build a thesaurs automatically\n",
    " to reduce dimensionality (down to a few \"concepts\")\n",
    " \n",
    "LSI uses Singular Value Decomposition (SVD)\n",
    "\n",
    "Through LSI we can get a ranking of the discovery concept which can allow us to throw away less important concept -- essentially dimensionality reduction\n",
    "\n",
    "--\n",
    "LSI helps in finding important concepts\n",
    "  EG transactions among peaple at a grocery store to turn data into information\n",
    "   eg contectuallise data into colums for eg vegetarians and meat eaters, grocery items ({bread,lettuce tomatoes},{beef,chicken,..,etc})\n",
    "   We get this data by creating matrices and analysing them.   \n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "We can figure what the concept is from the set of words and their corresponding weights\n",
    "\n",
    "SVD\n",
    "\n",
    "LDA\n",
    "-----\n",
    "\n",
    "Sort documents according how related or similiar they are with other documents in terms of content.\n",
    "\n",
    "Initially we need to feed knowledge to the machine -- training data\n",
    "\n",
    "Chosing the corpus is important tas the model assumes each document contains terms that are somehow related to the same topic\n",
    "For this reason we need to set the right \"window\" of context -- Which means we may need to remove some documents to make sampling easier\n",
    "    This may include removing meaningless documents for the topics eg files with meta data\n",
    "    \n",
    "Gather all the words in the (remining) corpus and sort them by popularity\n",
    "\n",
    "Sample the words  and remove others according to a meaningful criteria eg. model must contain 50,000 - 100,000 words, remove words that          appear in n no of, or x% of dictionaries, or if is in less than n no of, or x% of dictionaries.\n",
    "\n",
    "Perform a TF-IDF analysis on these words\n",
    "\n",
    "LDA brings together context, words and topics (@8:09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some visualization of the LDA results using pyLDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/yasin/opt/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el11811405267522043848292902705\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el11811405267522043848292902705_data = {\"mdsDat\": {\"x\": [-0.2284945193217148, -0.14962673296859094, 0.21956889574526514, 0.15855235654504066], \"y\": [-0.02862286892380212, -0.010718595573706495, -0.23570066753340413, 0.27504213203091277], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [26.89490920030767, 26.672350893586845, 23.91829819141068, 22.5144417146948]}, \"tinfo\": {\"Term\": [\"report\", \"question\", \"mine\", \"sydney\", \"charg\", \"council\", \"australian\", \"fund\", \"interview\", \"elect\", \"back\", \"win\", \"may\", \"labor\", \"trial\", \"union\", \"road\", \"cut\", \"call\", \"govern\", \"final\", \"lead\", \"stori\", \"begin\", \"want\", \"urg\", \"warn\", \"govt\", \"forc\", \"attack\", \"question\", \"council\", \"fund\", \"cut\", \"govern\", \"urg\", \"forc\", \"set\", \"want\", \"aussi\", \"support\", \"concern\", \"farmer\", \"approv\", \"group\", \"doctor\", \"claim\", \"hospit\", \"school\", \"mp\", \"citi\", \"coast\", \"team\", \"figur\", \"ban\", \"reduc\", \"titl\", \"men\", \"job\", \"seek\", \"disput\", \"draw\", \"next\", \"drug\", \"govt\", \"plan\", \"new\", \"say\", \"wa\", \"fear\", \"polic\", \"chang\", \"mine\", \"elect\", \"win\", \"road\", \"final\", \"warn\", \"commun\", \"club\", \"run\", \"rise\", \"station\", \"student\", \"drought\", \"perth\", \"woman\", \"rate\", \"found\", \"search\", \"debat\", \"stage\", \"fall\", \"turnbul\", \"held\", \"record\", \"minist\", \"world\", \"injuri\", \"telstra\", \"countri\", \"review\", \"qld\", \"econom\", \"friday\", \"eye\", \"debt\", \"polic\", \"call\", \"water\", \"help\", \"health\", \"take\", \"trade\", \"futur\", \"miss\", \"hit\", \"sydney\", \"charg\", \"australian\", \"interview\", \"trial\", \"stori\", \"lead\", \"murder\", \"market\", \"open\", \"shoot\", \"accus\", \"leav\", \"day\", \"test\", \"two\", \"busi\", \"success\", \"olymp\", \"bu\", \"risk\", \"point\", \"board\", \"arm\", \"pull\", \"full\", \"second\", \"appoint\", \"hope\", \"fourth\", \"latest\", \"sieg\", \"line\", \"sold\", \"man\", \"melbourn\", \"polic\", \"court\", \"us\", \"report\", \"back\", \"may\", \"labor\", \"union\", \"begin\", \"blaze\", \"car\", \"die\", \"drum\", \"work\", \"news\", \"pole\", \"dead\", \"judg\", \"abc\", \"great\", \"rural\", \"collaps\", \"resid\", \"state\", \"train\", \"garrett\", \"iraq\", \"pakistan\", \"big\", \"fail\", \"deni\", \"former\", \"bid\", \"italian\", \"singapor\", \"lock\", \"sunday\", \"paint\", \"attack\", \"pay\", \"crash\", \"call\", \"kill\", \"man\", \"us\"], \"Freq\": [373.0, 354.0, 336.0, 298.0, 284.0, 284.0, 259.0, 264.0, 229.0, 235.0, 203.0, 229.0, 202.0, 190.0, 195.0, 183.0, 206.0, 206.0, 366.0, 199.0, 195.0, 176.0, 175.0, 167.0, 188.0, 183.0, 179.0, 343.0, 180.0, 230.0, 353.68453628361857, 283.8701424896969, 264.1848854203118, 206.05884348423442, 198.9789345195002, 182.7043037126356, 179.8490866160094, 172.33295268559917, 187.95133801745854, 167.20028433227935, 162.58747289366755, 147.49804133009042, 142.86591835062825, 142.5370182689996, 135.91290278405657, 138.27967794759766, 132.44164367970444, 127.64868113379849, 120.17042506262108, 119.47560790056808, 124.83478533815351, 115.43679764355946, 125.00835792986855, 122.95920172163102, 112.60258811133134, 120.00873748440344, 135.00798428069598, 120.3063382223784, 107.58023838059391, 105.93106748336274, 112.26931948420095, 109.14911155690399, 109.46742434002253, 166.9552089296313, 232.09691651424075, 208.72650864354, 221.35777250194684, 162.21347400406498, 147.63067808072142, 133.9370738969322, 172.33692292254048, 127.50505877609692, 335.9587799994544, 234.9229379752981, 228.460584220895, 206.06477635270178, 194.36381761126765, 179.20480464405054, 147.21033285923096, 150.3376978993703, 155.47710377611173, 140.19484690462733, 137.63781622254217, 146.01957400792048, 152.74077563983457, 145.5293889631253, 127.95641629780911, 126.6250967033253, 120.22079704609357, 116.98847335836842, 111.62536297549417, 117.16429215397226, 109.05291406303644, 115.30180815198405, 113.86040748957578, 102.20254971031949, 102.14094050052499, 100.6604017434148, 116.17446724873199, 111.87380036191885, 95.22222624060271, 95.70108971706232, 167.02966523579474, 115.55362225828371, 111.96534132637878, 113.38134650685934, 104.52115211338298, 291.3567957543114, 185.1022905038952, 129.43053928715685, 137.6411037460431, 123.08717349574462, 125.754603804233, 121.01962702980131, 121.71001970733509, 118.86387809225563, 118.5900579662424, 298.2017744581249, 283.70366714300013, 258.8386132335069, 229.1014731031571, 194.48900158921617, 175.101908068431, 175.71947167810407, 163.84001643699767, 152.2594589663429, 146.69111393950226, 141.02694519896, 136.23381698688922, 133.54320000124522, 130.03723587856567, 122.43159139898005, 121.84557671293325, 114.17416055482788, 110.27963390121292, 110.42936845217241, 104.84639733976798, 113.75327208947061, 103.39009628278656, 104.2777511303282, 102.2756914024885, 97.05447594857205, 100.93212547064188, 93.29711455363518, 97.17986035239366, 95.8983524445291, 99.42386753002091, 101.51843724905169, 99.22110199226327, 99.90217480784547, 98.87287546292183, 229.19679404516748, 125.23374371367743, 217.12663978689355, 145.12002096410365, 113.43138394077336, 373.0589148713984, 202.46623584509035, 201.56236977717052, 189.39477977305594, 182.6027372881427, 166.60662902142417, 154.32312207162127, 134.5427240363761, 120.65986010048017, 121.39339483761034, 114.25395646220426, 109.06129247529746, 114.69027184699098, 106.26844377439316, 113.3677624311932, 101.14459639865032, 105.95943589347272, 93.38961613145145, 119.81741054549346, 90.92777124298217, 91.31956772285993, 89.96048267619891, 94.83102189295198, 79.60505626209708, 79.37382470429738, 75.22565016360949, 74.03145393363232, 72.51364323349422, 71.77295040715967, 71.99371641906026, 104.85209939207549, 103.55882466667215, 99.8780394697664, 96.69423325360934, 92.90561959047687, 187.18236552117494, 135.0677545522788, 156.60969864516142, 181.20713039040209, 143.07247737881121, 112.83751527627601, 100.27575172584612], \"Total\": [373.0, 354.0, 336.0, 298.0, 284.0, 284.0, 259.0, 264.0, 229.0, 235.0, 203.0, 229.0, 202.0, 190.0, 195.0, 183.0, 206.0, 206.0, 366.0, 199.0, 195.0, 176.0, 175.0, 167.0, 188.0, 183.0, 179.0, 343.0, 180.0, 230.0, 354.4710666415878, 284.57153135399386, 264.92665968933704, 206.77949438711866, 199.69712609417115, 183.40413456266666, 180.57340819443885, 173.0470394479409, 188.74415570578503, 167.93045279606446, 163.31542261370367, 148.20128514915663, 143.56688992985747, 143.26391653752864, 136.61381258003064, 138.99760261817076, 133.14972204782114, 128.35782621168914, 120.87733926440706, 120.1835202088521, 125.5854785386784, 116.13272728285807, 125.76211339127494, 123.72409912282359, 113.30936918158893, 120.76316971553435, 135.8637600123569, 121.07764273343288, 108.2785132882933, 106.64293893264212, 113.04516872672377, 109.94149928285921, 110.35434253221217, 205.2544960780731, 343.3738201008173, 322.71706513130545, 442.1847871576007, 329.5406462348013, 254.59506650624255, 184.5635146924691, 698.4186995904878, 169.20344558770597, 336.7068364603414, 235.6449184347956, 229.17801571515275, 206.7834400065476, 195.08218872034422, 179.9224779924347, 147.92024281079827, 151.07906912159092, 156.254400876117, 140.8982716877182, 138.35863423279673, 146.78950138520312, 153.5493185648518, 146.3035509209866, 128.67173435957244, 127.35002597831313, 120.93393403358682, 117.69716728238366, 112.33088200235626, 117.90784777225515, 109.76458408378295, 116.06411949507256, 114.63044684147567, 102.91155728853408, 102.84966574549519, 101.36773519592906, 117.009280207045, 112.69542678330295, 95.9229755733342, 96.41390633528646, 168.44569480784776, 116.47253374704978, 112.87887363743388, 114.3791320361192, 105.56566550524288, 698.4186995904878, 366.7856211783853, 180.51464442473818, 224.35734750505975, 164.4047756132905, 195.05124449323986, 161.48882436541422, 176.48414947704384, 166.20148487806307, 182.08867102724307, 298.9279808646179, 284.4208918094187, 259.6103846818179, 229.81031401753327, 195.22424071483968, 175.83519441778287, 176.45997470036784, 164.5543679880857, 152.98383817230567, 147.41864861651132, 141.75588441333684, 136.96050564801433, 134.26880985185392, 130.75488147416397, 123.1528086394165, 122.57638591548458, 114.90575011669583, 111.01013668880626, 111.17557115318557, 105.57997088368971, 114.57224828472748, 104.14863359573131, 105.04400425045198, 103.0300282686441, 97.77328359345633, 101.69254850999418, 94.00791010329161, 97.92277877834562, 96.63195107623227, 100.18970125300214, 102.32215161493747, 99.99688726284148, 100.73009450583093, 99.84095119839141, 385.37654759807407, 150.847991468743, 698.4186995904878, 257.2890344574373, 214.18833125928387, 373.7793166356604, 203.17085770726464, 202.26567207429284, 190.15048447367397, 183.34170494938243, 167.319867446579, 155.0628491389402, 135.23805634190686, 121.34878130266881, 122.09398140279612, 114.95340196676231, 109.748126922307, 115.4127384194589, 106.9659970362787, 114.116685006593, 101.82920215645252, 106.67839553665945, 94.07976121926816, 120.70825515698401, 91.62643778213382, 92.02607108931298, 90.66747656552748, 95.60820532938143, 80.29610212121915, 80.08261199503579, 75.93242590766533, 74.72853229860289, 73.2142404609576, 72.47276025743103, 72.70230595235348, 105.94181090569249, 104.63461385195062, 101.025725901229, 97.94350045694891, 94.82568823861736, 230.01188924090235, 162.42135316100854, 201.36120472204416, 366.7856211783853, 228.6270497057743, 385.37654759807407, 214.18833125928387], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3569, -4.5768, -4.6486, -4.8971, -4.9321, -5.0174, -5.0332, -5.0759, -4.9891, -5.1061, -5.1341, -5.2315, -5.2634, -5.2657, -5.3133, -5.296, -5.3391, -5.376, -5.4364, -5.4422, -5.3983, -5.4766, -5.3969, -5.4134, -5.5014, -5.4377, -5.32, -5.4353, -5.5471, -5.5625, -5.5044, -5.5326, -5.5297, -5.1076, -4.7781, -4.8843, -4.8255, -5.1364, -5.2306, -5.3279, -5.0758, -5.3771, -4.4, -4.7577, -4.7856, -4.8888, -4.9473, -5.0285, -5.2251, -5.2041, -5.1705, -5.2739, -5.2924, -5.2332, -5.1882, -5.2366, -5.3653, -5.3758, -5.4277, -5.4549, -5.5018, -5.4534, -5.5251, -5.4694, -5.482, -5.59, -5.5906, -5.6052, -5.4619, -5.4996, -5.6608, -5.6558, -5.0988, -5.4672, -5.4988, -5.4862, -5.5676, -4.5424, -4.9961, -5.3538, -5.2923, -5.4041, -5.3826, -5.421, -5.4153, -5.439, -5.4413, -4.4102, -4.4601, -4.5518, -4.6738, -4.8376, -4.9426, -4.9391, -5.0091, -5.0824, -5.1197, -5.159, -5.1936, -5.2136, -5.2402, -5.3004, -5.3052, -5.3703, -5.405, -5.4036, -5.4555, -5.374, -5.4695, -5.4609, -5.4803, -5.5327, -5.4936, -5.5722, -5.5314, -5.5447, -5.5086, -5.4878, -5.5106, -5.5038, -5.5142, -4.6734, -5.2778, -4.7275, -5.1304, -5.3768, -4.1258, -4.7369, -4.7414, -4.8037, -4.8402, -4.9319, -5.0085, -5.1456, -5.2545, -5.2485, -5.3091, -5.3556, -5.3053, -5.3815, -5.3169, -5.431, -5.3845, -5.5107, -5.2615, -5.5374, -5.5331, -5.5481, -5.4954, -5.6704, -5.6733, -5.727, -5.743, -5.7637, -5.774, -5.7709, -5.395, -5.4074, -5.4436, -5.476, -5.5159, -4.8154, -5.1417, -4.9938, -4.8479, -5.0842, -5.3216, -5.4396], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.311, 1.3108, 1.3104, 1.3097, 1.3096, 1.3094, 1.3092, 1.3091, 1.309, 1.3089, 1.3088, 1.3085, 1.3083, 1.3081, 1.3081, 1.3081, 1.3079, 1.3077, 1.3074, 1.3073, 1.3072, 1.3072, 1.3072, 1.307, 1.307, 1.307, 1.3069, 1.3068, 1.3068, 1.3065, 1.3063, 1.306, 1.3052, 1.1067, 0.9216, 0.8775, 0.6213, 0.6044, 0.7683, 0.9926, -0.0861, 1.0303, 1.3193, 1.3185, 1.3184, 1.3181, 1.3179, 1.3175, 1.3167, 1.3166, 1.3166, 1.3165, 1.3163, 1.3163, 1.3163, 1.3162, 1.316, 1.3158, 1.3156, 1.3155, 1.3152, 1.3152, 1.315, 1.315, 1.3148, 1.3146, 1.3146, 1.3145, 1.3144, 1.3142, 1.3142, 1.3141, 1.3131, 1.3136, 1.3134, 1.3128, 1.3116, 0.4473, 0.6377, 0.9889, 0.833, 1.0321, 0.8826, 1.0331, 0.95, 0.9863, 0.8927, 1.4281, 1.428, 1.4275, 1.4274, 1.4268, 1.4263, 1.4263, 1.4262, 1.4258, 1.4256, 1.4254, 1.4252, 1.4251, 1.425, 1.4247, 1.4245, 1.4241, 1.4239, 1.4238, 1.4236, 1.4234, 1.4232, 1.4232, 1.4232, 1.4231, 1.423, 1.4229, 1.4229, 1.4229, 1.4229, 1.4226, 1.4227, 1.4223, 1.4208, 0.9109, 1.2444, 0.2622, 0.8579, 0.7949, 1.4891, 1.4875, 1.4875, 1.487, 1.487, 1.4867, 1.4862, 1.4859, 1.4853, 1.4853, 1.4849, 1.4847, 1.4847, 1.4845, 1.4844, 1.4843, 1.4843, 1.4837, 1.4836, 1.4834, 1.4833, 1.4832, 1.4829, 1.4824, 1.4821, 1.4817, 1.4816, 1.4814, 1.4813, 1.4812, 1.4807, 1.4807, 1.4796, 1.4782, 1.4706, 1.285, 1.3066, 1.2397, 0.7859, 1.0223, 0.2627, 0.7321]}, \"token.table\": {\"Topic\": [4, 3, 3, 1, 3, 3, 4, 1, 3, 4, 1, 4, 4, 4, 4, 3, 3, 3, 2, 4, 4, 1, 2, 3, 1, 1, 2, 1, 4, 2, 1, 1, 2, 1, 3, 2, 4, 1, 3, 4, 2, 2, 4, 4, 1, 1, 1, 2, 1, 2, 4, 2, 2, 2, 4, 2, 1, 1, 4, 1, 2, 1, 4, 2, 3, 2, 3, 1, 1, 2, 4, 1, 1, 2, 4, 1, 2, 3, 2, 1, 2, 3, 2, 4, 3, 1, 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 3, 3, 3, 4, 2, 3, 4, 3, 4, 2, 3, 1, 2, 2, 2, 3, 1, 3, 1, 2, 3, 4, 4, 1, 3, 3, 1, 2, 3, 4, 4, 2, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 3, 2, 4, 1, 2, 2, 1, 4, 4, 2, 2, 3, 2, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 1, 3, 3, 3, 4, 3, 2, 4, 2, 3, 2, 3, 2, 4, 1, 3, 2, 3, 1, 2, 3, 1, 1, 2, 4, 3, 2, 3, 4, 1, 3, 4, 1, 2, 1, 2, 1, 2, 2, 2, 4, 2], \"Freq\": [0.9918569316179211, 0.9929869881578649, 0.9905764645380991, 0.998157829662157, 0.9900026401433341, 0.1825992566671691, 0.8130014523038244, 0.9944592968066703, 0.9976488433520639, 0.9942370784842004, 0.997269694608456, 0.9980882877122699, 0.990339976935343, 0.9877203197906627, 0.9931456880558935, 0.9900612675811291, 0.9945068095886422, 0.9921174517743806, 0.5043818222907536, 0.49347626937635886, 0.9982397237261011, 0.7564857769616262, 0.2423118504330209, 0.9985201797000878, 0.9953380076622627, 0.9913651937823181, 0.9928575869055529, 0.9902462698555322, 0.9941325043919912, 0.9937787905609692, 0.9918942325773518, 0.9979916074131713, 0.9903779509776719, 0.43530809712190777, 0.5635685185953271, 0.2185127967462099, 0.7796933883898853, 0.9962303109917691, 0.9942267434633932, 0.9909691204397313, 0.9970543986083068, 0.9946415768561154, 0.9970737870172697, 0.9971257947634523, 0.9907544149078111, 0.9928228789606451, 0.9914363612557537, 0.9964225268468397, 0.813624077381856, 0.18513601760784748, 0.9910398416840303, 0.9959429598391325, 0.9972631769907058, 0.987942450588944, 0.9902509486511553, 0.9930343280561301, 0.9960513881011531, 0.7260373222913472, 0.27090944861617433, 0.9941474690221445, 0.9944526523541544, 0.9968245147490299, 0.9934767179316513, 0.9922773203315502, 0.9881255135196194, 0.9922140112749812, 0.9931897811576025, 0.9965022029477, 0.30597648661374005, 0.6912802104977089, 0.9936385655678182, 0.9965090829907968, 0.675648481098189, 0.32326285087025425, 0.9936407410962014, 0.9955069508094501, 0.748153449564738, 0.249384483188246, 0.9945001798488361, 0.37440271483912785, 0.6150901743785672, 0.008914350353312568, 0.6535277528726424, 0.3459852809325754, 0.9934602264655329, 0.9972122758522023, 0.9913743576128398, 0.99647398759713, 0.9963123724141412, 0.99111011131827, 0.997427806497936, 0.9902145334266546, 0.3717845290370871, 0.6254727959094524, 0.9939496106104676, 0.9968515945975237, 0.9973933199234054, 0.997997972484075, 0.9927519723930303, 0.9898468841270012, 0.11157918214796653, 0.5942240165554497, 0.29321971122605156, 0.9935690058240167, 0.9986865192122405, 0.1657297505693353, 0.8286487528466766, 0.9910995728930283, 0.9979007362375767, 0.9917387602639594, 0.7159984165442724, 0.2827892905679059, 0.9901523918853815, 0.9966310952734733, 0.49979105210879315, 0.1718738459740646, 0.12212089056051958, 0.20579631557420894, 0.9931832374429806, 0.9877273290644014, 0.9894259940291579, 0.9971601380121156, 0.010545665616300312, 0.010545665616300312, 0.010545665616300312, 0.980746902315929, 0.9864813101363015, 0.1662343003215523, 0.8311715016077614, 0.9979251978569507, 0.6476261176797798, 0.3532506096435163, 0.9889712082043255, 0.9964238053345651, 0.24627061116898907, 0.41665551075683616, 0.31070187571901525, 0.025772505820010484, 0.992091054273357, 0.9914174428174201, 0.005936631394116288, 0.9986710716729269, 0.9972514651989727, 0.9911423234420762, 0.9936804431571973, 0.9979150354206998, 0.9931631328544788, 0.9957069851122196, 0.9936246791606564, 0.9950053499578243, 0.9962113019953494, 0.9919720605046413, 0.9885229170942345, 0.4915933796056625, 0.2640038520104484, 0.11834655434951134, 0.1274501354533199, 0.9927419045641965, 0.9940766010050948, 0.9892784543110876, 0.9939711063941307, 0.9939493940417519, 0.9946677034504415, 0.9900308170571233, 0.009557066855667073, 0.9939349529893755, 0.9915770914810259, 0.9923003617705862, 0.9888502130193393, 0.9974079374606046, 0.9952501294149426, 0.9946215405205899, 0.990900500450351, 0.010209967944116417, 0.9903668905792924, 0.9980686293513763, 0.996895637330658, 0.6459840865274097, 0.35375319024120055, 0.9939400398838414, 0.993829148146001, 0.9906392013941651, 0.9936424546746069, 0.24769515882714377, 0.7492778554521099, 0.9926381918763882, 0.9937290537775588, 0.9908316239359595, 0.9952977409867346, 0.998136239927098, 0.9977964806320734, 0.527573091099948, 0.4668788416813699, 0.5813152706805146, 0.4163474235955037, 0.9960573311369438, 0.9948729141420924, 0.28252555443646227, 0.7146234612216399, 0.9948598223460625, 0.9947794722522719, 0.9917061874598719, 0.9963722658376625], \"Term\": [\"abc\", \"accus\", \"appoint\", \"approv\", \"arm\", \"attack\", \"attack\", \"aussi\", \"australian\", \"back\", \"ban\", \"begin\", \"bid\", \"big\", \"blaze\", \"board\", \"bu\", \"busi\", \"call\", \"call\", \"car\", \"chang\", \"chang\", \"charg\", \"citi\", \"claim\", \"club\", \"coast\", \"collaps\", \"commun\", \"concern\", \"council\", \"countri\", \"court\", \"court\", \"crash\", \"crash\", \"cut\", \"day\", \"dead\", \"debat\", \"debt\", \"deni\", \"die\", \"disput\", \"doctor\", \"draw\", \"drought\", \"drug\", \"drug\", \"drum\", \"econom\", \"elect\", \"eye\", \"fail\", \"fall\", \"farmer\", \"fear\", \"fear\", \"figur\", \"final\", \"forc\", \"former\", \"found\", \"fourth\", \"friday\", \"full\", \"fund\", \"futur\", \"futur\", \"garrett\", \"govern\", \"govt\", \"govt\", \"great\", \"group\", \"health\", \"health\", \"held\", \"help\", \"help\", \"help\", \"hit\", \"hit\", \"hope\", \"hospit\", \"injuri\", \"interview\", \"iraq\", \"italian\", \"job\", \"judg\", \"kill\", \"kill\", \"labor\", \"latest\", \"lead\", \"leav\", \"line\", \"lock\", \"man\", \"man\", \"man\", \"market\", \"may\", \"melbourn\", \"melbourn\", \"men\", \"mine\", \"minist\", \"miss\", \"miss\", \"mp\", \"murder\", \"new\", \"new\", \"new\", \"new\", \"news\", \"next\", \"olymp\", \"open\", \"paint\", \"paint\", \"paint\", \"paint\", \"pakistan\", \"pay\", \"pay\", \"perth\", \"plan\", \"plan\", \"point\", \"pole\", \"polic\", \"polic\", \"polic\", \"polic\", \"pull\", \"qld\", \"qld\", \"question\", \"rate\", \"record\", \"reduc\", \"report\", \"resid\", \"review\", \"rise\", \"risk\", \"road\", \"run\", \"rural\", \"say\", \"say\", \"say\", \"say\", \"school\", \"search\", \"second\", \"seek\", \"set\", \"shoot\", \"sieg\", \"singapor\", \"singapor\", \"sold\", \"stage\", \"state\", \"station\", \"stori\", \"student\", \"success\", \"sunday\", \"sunday\", \"support\", \"sydney\", \"take\", \"take\", \"team\", \"telstra\", \"test\", \"titl\", \"trade\", \"trade\", \"train\", \"trial\", \"turnbul\", \"two\", \"union\", \"urg\", \"us\", \"us\", \"wa\", \"wa\", \"want\", \"warn\", \"water\", \"water\", \"win\", \"woman\", \"work\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el11811405267522043848292902705\", ldavis_el11811405267522043848292902705_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el11811405267522043848292902705\", ldavis_el11811405267522043848292902705_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el11811405267522043848292902705\", ldavis_el11811405267522043848292902705_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.228495 -0.028623       1        1  26.894909\n",
       "1     -0.149627 -0.010719       2        1  26.672351\n",
       "3      0.219569 -0.235701       3        1  23.918298\n",
       "0      0.158552  0.275042       4        1  22.514442, topic_info=          Term        Freq       Total Category  logprob  loglift\n",
       "1133    report  373.000000  373.000000  Default  30.0000  30.0000\n",
       "647   question  354.000000  354.000000  Default  29.0000  29.0000\n",
       "1043      mine  336.000000  336.000000  Default  28.0000  28.0000\n",
       "886     sydney  298.000000  298.000000  Default  27.0000  27.0000\n",
       "86       charg  284.000000  284.000000  Default  26.0000  26.0000\n",
       "...        ...         ...         ...      ...      ...      ...\n",
       "0        crash  156.609699  201.361205   Topic4  -4.9938   1.2397\n",
       "177       call  181.207130  366.785621   Topic4  -4.8479   0.7859\n",
       "688       kill  143.072477  228.627050   Topic4  -5.0842   1.0223\n",
       "139        man  112.837515  385.376548   Topic4  -5.3216   0.2627\n",
       "124         us  100.275752  214.188331   Topic4  -5.4396   0.7321\n",
       "\n",
       "[198 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "496       4  0.991857      abc\n",
       "101       3  0.992987    accus\n",
       "833       3  0.990576  appoint\n",
       "68        1  0.998158   approv\n",
       "1411      3  0.990003      arm\n",
       "...     ...       ...      ...\n",
       "600       2  0.714623    water\n",
       "500       2  0.994860      win\n",
       "241       2  0.994779    woman\n",
       "279       4  0.991706     work\n",
       "151       2  0.996372    world\n",
       "\n",
       "[203 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 4, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: show visualization results of the LDA\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda,corpus,dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your results, you can try to fine tune the algorithm: number of topics, hyperparameters...\n",
    "And check with others their results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
